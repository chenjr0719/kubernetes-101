{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kubernetes 101 Demonstrate and learn the common knowledge of Kubernetes. In this practice, you will learn: How to set up the local Kubernetes cluster How to deploy workloads to Kubernetes cluster Let's starting from the requirements first.","title":"Home"},{"location":"#kubernetes-101","text":"Demonstrate and learn the common knowledge of Kubernetes. In this practice, you will learn: How to set up the local Kubernetes cluster How to deploy workloads to Kubernetes cluster Let's starting from the requirements first.","title":"Kubernetes 101"},{"location":"requirements/","text":"Requirements Docker Before you start this practice, please make sure you have Docker on your laptop/workstation. If you are using Docker Desktop , please increase the RAM of your VM to at least 8GB . If you are using Docker on any Linux distro, please make sure you can use the docker command without sudo. Please check here to get more information: https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user Kubectl To access the kubernetes cluster, you will need to use the tool called kubectl . About how to install the kubectl , please check the official doc here: https://kubernetes.io/docs/tasks/tools/#kubectl Helm To deploy third-party applications/services to K8s, we will use Helm to help us. Please following the doc to setup the helm CLI first. https://helm.sh/docs/intro/install/#from-script","title":"Requirements"},{"location":"requirements/#requirements","text":"","title":"Requirements"},{"location":"requirements/#docker","text":"Before you start this practice, please make sure you have Docker on your laptop/workstation. If you are using Docker Desktop , please increase the RAM of your VM to at least 8GB . If you are using Docker on any Linux distro, please make sure you can use the docker command without sudo. Please check here to get more information: https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user","title":"Docker"},{"location":"requirements/#kubectl","text":"To access the kubernetes cluster, you will need to use the tool called kubectl . About how to install the kubectl , please check the official doc here: https://kubernetes.io/docs/tasks/tools/#kubectl","title":"Kubectl"},{"location":"requirements/#helm","text":"To deploy third-party applications/services to K8s, we will use Helm to help us. Please following the doc to setup the helm CLI first. https://helm.sh/docs/intro/install/#from-script","title":"Helm"},{"location":"setup/","text":"Setup Kubernetes Let's starting from setup the local kubernetes first. Kind The Kind , means Kubernetes IN Docker . Is a tool for running local Kubernetes clusters using Docker container. This is a very convenience tool when you develop application on K8s. To install the Kind CLI, please follow the instruction from its official doc: https://kind.sigs.K8s.io/docs/user/quick-start/#installation After you finish the installation, please use following command to create the K8s cluster at your local env: #!/usr/bin/env bash cat <<EOF | kind create cluster --wait 5m --config=- kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 containerdConfigPatches: - |- [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"localhost:32000\"] endpoint = [\"http://localhost:32000\"] nodes: - role: control-plane kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \"ingress-ready=true\" extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP - containerPort: 32000 hostPort: 32000 protocol: TCP EOF This configuration will expose 80 and 443 port for the ingress and the 32000 for the NodePort usage. After the kind cluster is ready, you can use the kubectl to access your local cluster. kubectl get nodes Nginx Ingress In the kubernetes, we will expose the service to the external network by using NodePort or Ingress . Usually, we only use the Ingress if your service is a HTTP based service. There are many solution to use Ingress with your kubernetes. In this practice, we will use Nginx Ingress . To install the Nginx Ingrss, we will use its helm chart. Here is the command to install the Nginx ingress with helm chart: #!/usr/bin/env bash helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \\ --namespace ingress-nginx \\ --create-namespace \\ --set controller.hostPort.enabled=true \\ --set controller.ingressClassResource.default=true \\ -f - << EOF controller: hostPort: enabled: true ingressClassResource: default: true service: type: NodePort watchIngressWithoutClass: true EOF kubectl -n ingress-nginx rollout status deployment ingress-nginx-controller Before you move to next step, we need to declare our domain name first. For the development purpose, using the nip.io is recommended. Please export the domain with nip.io like this: export DOMAIN=127.0.0.1.nip.io Rancher Now, let's introduce the Rancher . The Rancher , is a nice tool that can help you manage and operate your K8s cluster. To install Rancher, you will need to instal the cert-manager first. Here is the command to install cert-manager and Rancher . #!/usr/bin/env bash # Install Cert-manager helm repo add jetstack https://charts.jetstack.io helm repo update kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.15.0/cert-manager.crds.yaml helm upgrade --install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v0.15.0 kubectl -n cert-manager rollout status deployment cert-manager sleep 30 # Prevent unexpected issue # Install Rancher helm repo add rancher-latest https://releases.rancher.com/server-charts/latest helm repo update helm upgrade --install rancher rancher-latest/rancher \\ --version v2.5.11 \\ --namespace cattle-system \\ --create-namespace \\ --set hostname=rancher.$DOMAIN \\ --set replicas=1 kubectl -n cattle-system rollout status deployment rancher Since we declare the $DOMAIN as 127.0.0.1.nip.io , we can use the URL https://rancher.127.0.0.1.nip.io to access the rancher on the local K8s cluster. It is exposed by the Nginx ingress with nip.io to use the subdomain. If you want to try it on the VM with AWS or GCP, you can replace it by the external IP of your VM. After the installation is finished, you can access your rancher and see the screen: Please set up the password of the admin user and continue with multipl cluster view. Now, you can see the screen of clusters. Since we only install rancher with local cluster, so it will looks like this: You can explore the Rancher and try to use it to operate your K8s cluster. Wrap Up We finished the set up of K8s. You can use it as the playground to develop or learn the K8s. With the Rancher, you can use the UI to help you learn how to operate the K8s. It would be helpful if you spend some time to explore it. Now, let's move to the next step to learn how to deploy workloads to your K8s.","title":"Setup"},{"location":"setup/#setup-kubernetes","text":"Let's starting from setup the local kubernetes first.","title":"Setup Kubernetes"},{"location":"setup/#kind","text":"The Kind , means Kubernetes IN Docker . Is a tool for running local Kubernetes clusters using Docker container. This is a very convenience tool when you develop application on K8s. To install the Kind CLI, please follow the instruction from its official doc: https://kind.sigs.K8s.io/docs/user/quick-start/#installation After you finish the installation, please use following command to create the K8s cluster at your local env: #!/usr/bin/env bash cat <<EOF | kind create cluster --wait 5m --config=- kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 containerdConfigPatches: - |- [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"localhost:32000\"] endpoint = [\"http://localhost:32000\"] nodes: - role: control-plane kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \"ingress-ready=true\" extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP - containerPort: 32000 hostPort: 32000 protocol: TCP EOF This configuration will expose 80 and 443 port for the ingress and the 32000 for the NodePort usage. After the kind cluster is ready, you can use the kubectl to access your local cluster. kubectl get nodes","title":"Kind"},{"location":"setup/#nginx-ingress","text":"In the kubernetes, we will expose the service to the external network by using NodePort or Ingress . Usually, we only use the Ingress if your service is a HTTP based service. There are many solution to use Ingress with your kubernetes. In this practice, we will use Nginx Ingress . To install the Nginx Ingrss, we will use its helm chart. Here is the command to install the Nginx ingress with helm chart: #!/usr/bin/env bash helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \\ --namespace ingress-nginx \\ --create-namespace \\ --set controller.hostPort.enabled=true \\ --set controller.ingressClassResource.default=true \\ -f - << EOF controller: hostPort: enabled: true ingressClassResource: default: true service: type: NodePort watchIngressWithoutClass: true EOF kubectl -n ingress-nginx rollout status deployment ingress-nginx-controller Before you move to next step, we need to declare our domain name first. For the development purpose, using the nip.io is recommended. Please export the domain with nip.io like this: export DOMAIN=127.0.0.1.nip.io","title":"Nginx Ingress"},{"location":"setup/#rancher","text":"Now, let's introduce the Rancher . The Rancher , is a nice tool that can help you manage and operate your K8s cluster. To install Rancher, you will need to instal the cert-manager first. Here is the command to install cert-manager and Rancher . #!/usr/bin/env bash # Install Cert-manager helm repo add jetstack https://charts.jetstack.io helm repo update kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.15.0/cert-manager.crds.yaml helm upgrade --install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v0.15.0 kubectl -n cert-manager rollout status deployment cert-manager sleep 30 # Prevent unexpected issue # Install Rancher helm repo add rancher-latest https://releases.rancher.com/server-charts/latest helm repo update helm upgrade --install rancher rancher-latest/rancher \\ --version v2.5.11 \\ --namespace cattle-system \\ --create-namespace \\ --set hostname=rancher.$DOMAIN \\ --set replicas=1 kubectl -n cattle-system rollout status deployment rancher Since we declare the $DOMAIN as 127.0.0.1.nip.io , we can use the URL https://rancher.127.0.0.1.nip.io to access the rancher on the local K8s cluster. It is exposed by the Nginx ingress with nip.io to use the subdomain. If you want to try it on the VM with AWS or GCP, you can replace it by the external IP of your VM. After the installation is finished, you can access your rancher and see the screen: Please set up the password of the admin user and continue with multipl cluster view. Now, you can see the screen of clusters. Since we only install rancher with local cluster, so it will looks like this: You can explore the Rancher and try to use it to operate your K8s cluster.","title":"Rancher"},{"location":"setup/#wrap-up","text":"We finished the set up of K8s. You can use it as the playground to develop or learn the K8s. With the Rancher, you can use the UI to help you learn how to operate the K8s. It would be helpful if you spend some time to explore it. Now, let's move to the next step to learn how to deploy workloads to your K8s.","title":"Wrap Up"},{"location":"workloads/","text":"Deploy Workloads In the following examples, we will use a docker image which provided by Katacoda to deploy an example HTTP server. Pod The Pod is the minimum component to run containers on Kubernetes. And when you need to scale containers on K8s (ex: Scale HTTP API to 10 instances), usually the target is Pod. Each pod contains at least one container but it also supports multiple containers in a Pod. We call those additional containers as Sidecar containers. For example, you can run a container for your workload (ex: HTTP API, application) and write the logs to a local path in the container. Then, use the sidecar container to stream the logs from the local path to the log aggregator (ex: fluentd). To create a Pod, you can use the command like: # Without -n/--namespace, this pod will run in the default namespace kubectl run -it ubuntu --image ubuntu:20.04 # You can operate with bash now. # Let's exit the bash first, and use exec to enter pod again exit kubectl exec -it ubuntu -- bash After you create the pod, you can also check the Rancher. You can found it under the deault project: And the pod can be found under Workloads tab: Before we move to next step, please delete this pod with: kubectl delete pod ubuntu Deployment You can create a Pod to run your workload on Kubernetes. However, when you need to scale it, you might need to create dozen of Pods. It could be a very tedious process if you do it manually. So, there is another component Deployment to help you make this process more easily. The Deployment , which contains a template to create Pods (aka, Pod Template). And you can simply modify the number of replicas to scale your workload. But remember, it is suitable to run with a stateless workload (ex: RESTful API, if you implement with correct design). So, you can imagine that you have an HTTP API server. With Deployment, you can simply scale it to handle more incoming HTTP requests. To better manage the Deployment, we usually use YAML to manage the configuration. Let's create the YAML file first: cat << EOF > deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: http-server name: http-server namespace: default spec: replicas: 1 selector: matchLabels: app: http-server template: metadata: labels: app: http-server spec: containers: - image: katacoda/docker-http-server imagePullPolicy: Always name: http-server ports: - containerPort: 80 EOF And then apply it to the K8s: kubectl apply -f deployment.yaml You can check with the CLI: kubectl get deployment Or with Rancher, there is a Deployment has been created: And you can check the detail by click the link and see all the details like this: There is only one pod in this deployment now. Let's try to scale it out and see what will happened on K8s. Click the + button and you can see there is a new pod has been created. You can also check with kubectl to see there are two pods in the default namespace: kubectl get pods And you can apply the YAML again, it will make the number of pods to 1. Remember, the YAML file represents the desired state of your workload. It will change the state of K8s to fulfill your desired state. If you want to delete the deployment, you can: kubectl delete deployment http-server # OR kubectl delete -f deployment.yaml Now, you might have a question, if we can have multiple instances of API server, how can we load balancing it? So, let\u2019s talk about Service . Service The Service is an abstract layer to help us forward the network traffic to a set of Pods (it could be Deployment). By default, it will forward the traffic to different Pods without any addtional configuration. Usually, we will combine Deployment with Service, after scaling your workload by changing the number of replicas, you don\u2019t have to worry about the load balance since the Kubernetes already provide Service to resolve this problem. The K8s Service provides three different types to publish the service: ClusterIP: Only provide internal IP in the cluster. NodePort: Based on ClusterIP but Kubernetes will also expose the NodePort. The traffic to node external IP plus the NodePort will be forwarded to your workload if you use the NodePort type. Load Balancer: Based on NodePort but plus the cloud provider\u2019s support to allocate like AWS ELB to serving your workload. Now, let's create a service: cat << EOF > service.yaml apiVersion: v1 kind: Service metadata: labels: app: http-server name: http-server namespace: default spec: clusterIP: ports: - name: default port: 80 protocol: TCP targetPort: 80 selector: app: http-server type: ClusterIP EOF And apply it: kubectl apply -f service.yaml And you can see there is a new service has been created on K8s. You can check it with Rancher in the Service Discovery tab: Or the CLI: kubectl get services Since we use the service type ClusterIP now so we cannot access from out of K8s. Let's apply some change on it: CLUSTER_IP=$(kubectl get service http-server -o jsonpath='{.spec.clusterIP}') cat << EOF > service.yaml apiVersion: v1 kind: Service metadata: labels: app: http-server name: http-server namespace: default spec: clusterIP: $CLUSTER_IP ports: - name: default port: 80 protocol: TCP targetPort: 80 nodePort: 32000 selector: app: http-server type: NodePort EOF And apply it again, you can see the Rancher shows the port has been allocated with this service: You can also found it on the Workload tab: Now, let's access the HTTP service with this NodePort http://localhost:32000 . You should get the response similar to this: The response will display the pod name. So we can scale out the number of pods and you can see the K8s service will load balancing the traffic automatically. As you can see, we have two pods and the service will load balancing the traffic to this two pods. Ingress In the above example, we can use the NodePort to access the HTTP server on the K8s. However, exposing your workload to the outside world is still difficult. Using NodePort needs to expose 30000 - 32767 ports and it could be a serious security issue. On the other hand, using the Load Balancer needs support from cloud provider/Infrastructure so it might not be an ideal solution. So, we have the Ingress to resolve our problem. The Ingress , is an abstract layer to help us manage reverse proxy. You can find different implementations for Ingress controllers like Nginx, Traefik, Kong, and other popular solutions. Using the ingress-nginx is a common and simple one. You can create ingress with subdomain and path. Then, the ingress controller will forward the traffic to your workload. You only need to create service with type: ClusterIP and don\u2019t worry about other management issues. Furthermore, if you can have a wildcard FQDN, you can simply create multiple Ingresses to serve your micro-services without too much operation effort. Let's revert the configuration of service first: CLUSTER_IP=$(kubectl get service http-server -o jsonpath='{.spec.clusterIP}') cat << EOF > service.yaml apiVersion: v1 kind: Service metadata: labels: app: http-server name: http-server namespace: default spec: clusterIP: $CLUSTER_IP ports: - name: default port: 80 protocol: TCP targetPort: 80 selector: app: http-server type: ClusterIP EOF And apply it: kubectl apply -f service.yaml Now, let's create the ingress for this service: cat << EOF >> ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: labels: app: http-server name: http-server namespace: default spec: rules: - host: example.127.0.0.1.nip.io http: paths: - backend: serviceName: http-server servicePort: 80 pathType: ImplementationSpecific EOF And apply it: kubectl apply -f ingress.yaml You can check with CLI: kubectl get ingress Or using the Rancher, navigate to Load Balancing tab, you can see your ingress like this: Now, you can access the URL http://example.127.0.0.1.nip.io to access the deployment with ingress and it should looks like: Wrap Up In this example, you can learn how to deploy a HTTP server and expose it for external access. Besides, with the K8s deployment, you can also scale the HTTP server to multiple instances which can handle large workloads with minimum operation efforts. There still a lot of topics to optimize the configuration to make it production ready. We might have more example in the future.","title":"Workloads"},{"location":"workloads/#deploy-workloads","text":"In the following examples, we will use a docker image which provided by Katacoda to deploy an example HTTP server.","title":"Deploy Workloads"},{"location":"workloads/#pod","text":"The Pod is the minimum component to run containers on Kubernetes. And when you need to scale containers on K8s (ex: Scale HTTP API to 10 instances), usually the target is Pod. Each pod contains at least one container but it also supports multiple containers in a Pod. We call those additional containers as Sidecar containers. For example, you can run a container for your workload (ex: HTTP API, application) and write the logs to a local path in the container. Then, use the sidecar container to stream the logs from the local path to the log aggregator (ex: fluentd). To create a Pod, you can use the command like: # Without -n/--namespace, this pod will run in the default namespace kubectl run -it ubuntu --image ubuntu:20.04 # You can operate with bash now. # Let's exit the bash first, and use exec to enter pod again exit kubectl exec -it ubuntu -- bash After you create the pod, you can also check the Rancher. You can found it under the deault project: And the pod can be found under Workloads tab: Before we move to next step, please delete this pod with: kubectl delete pod ubuntu","title":"Pod"},{"location":"workloads/#deployment","text":"You can create a Pod to run your workload on Kubernetes. However, when you need to scale it, you might need to create dozen of Pods. It could be a very tedious process if you do it manually. So, there is another component Deployment to help you make this process more easily. The Deployment , which contains a template to create Pods (aka, Pod Template). And you can simply modify the number of replicas to scale your workload. But remember, it is suitable to run with a stateless workload (ex: RESTful API, if you implement with correct design). So, you can imagine that you have an HTTP API server. With Deployment, you can simply scale it to handle more incoming HTTP requests. To better manage the Deployment, we usually use YAML to manage the configuration. Let's create the YAML file first: cat << EOF > deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: http-server name: http-server namespace: default spec: replicas: 1 selector: matchLabels: app: http-server template: metadata: labels: app: http-server spec: containers: - image: katacoda/docker-http-server imagePullPolicy: Always name: http-server ports: - containerPort: 80 EOF And then apply it to the K8s: kubectl apply -f deployment.yaml You can check with the CLI: kubectl get deployment Or with Rancher, there is a Deployment has been created: And you can check the detail by click the link and see all the details like this: There is only one pod in this deployment now. Let's try to scale it out and see what will happened on K8s. Click the + button and you can see there is a new pod has been created. You can also check with kubectl to see there are two pods in the default namespace: kubectl get pods And you can apply the YAML again, it will make the number of pods to 1. Remember, the YAML file represents the desired state of your workload. It will change the state of K8s to fulfill your desired state. If you want to delete the deployment, you can: kubectl delete deployment http-server # OR kubectl delete -f deployment.yaml Now, you might have a question, if we can have multiple instances of API server, how can we load balancing it? So, let\u2019s talk about Service .","title":"Deployment"},{"location":"workloads/#service","text":"The Service is an abstract layer to help us forward the network traffic to a set of Pods (it could be Deployment). By default, it will forward the traffic to different Pods without any addtional configuration. Usually, we will combine Deployment with Service, after scaling your workload by changing the number of replicas, you don\u2019t have to worry about the load balance since the Kubernetes already provide Service to resolve this problem. The K8s Service provides three different types to publish the service: ClusterIP: Only provide internal IP in the cluster. NodePort: Based on ClusterIP but Kubernetes will also expose the NodePort. The traffic to node external IP plus the NodePort will be forwarded to your workload if you use the NodePort type. Load Balancer: Based on NodePort but plus the cloud provider\u2019s support to allocate like AWS ELB to serving your workload. Now, let's create a service: cat << EOF > service.yaml apiVersion: v1 kind: Service metadata: labels: app: http-server name: http-server namespace: default spec: clusterIP: ports: - name: default port: 80 protocol: TCP targetPort: 80 selector: app: http-server type: ClusterIP EOF And apply it: kubectl apply -f service.yaml And you can see there is a new service has been created on K8s. You can check it with Rancher in the Service Discovery tab: Or the CLI: kubectl get services Since we use the service type ClusterIP now so we cannot access from out of K8s. Let's apply some change on it: CLUSTER_IP=$(kubectl get service http-server -o jsonpath='{.spec.clusterIP}') cat << EOF > service.yaml apiVersion: v1 kind: Service metadata: labels: app: http-server name: http-server namespace: default spec: clusterIP: $CLUSTER_IP ports: - name: default port: 80 protocol: TCP targetPort: 80 nodePort: 32000 selector: app: http-server type: NodePort EOF And apply it again, you can see the Rancher shows the port has been allocated with this service: You can also found it on the Workload tab: Now, let's access the HTTP service with this NodePort http://localhost:32000 . You should get the response similar to this: The response will display the pod name. So we can scale out the number of pods and you can see the K8s service will load balancing the traffic automatically. As you can see, we have two pods and the service will load balancing the traffic to this two pods.","title":"Service"},{"location":"workloads/#ingress","text":"In the above example, we can use the NodePort to access the HTTP server on the K8s. However, exposing your workload to the outside world is still difficult. Using NodePort needs to expose 30000 - 32767 ports and it could be a serious security issue. On the other hand, using the Load Balancer needs support from cloud provider/Infrastructure so it might not be an ideal solution. So, we have the Ingress to resolve our problem. The Ingress , is an abstract layer to help us manage reverse proxy. You can find different implementations for Ingress controllers like Nginx, Traefik, Kong, and other popular solutions. Using the ingress-nginx is a common and simple one. You can create ingress with subdomain and path. Then, the ingress controller will forward the traffic to your workload. You only need to create service with type: ClusterIP and don\u2019t worry about other management issues. Furthermore, if you can have a wildcard FQDN, you can simply create multiple Ingresses to serve your micro-services without too much operation effort. Let's revert the configuration of service first: CLUSTER_IP=$(kubectl get service http-server -o jsonpath='{.spec.clusterIP}') cat << EOF > service.yaml apiVersion: v1 kind: Service metadata: labels: app: http-server name: http-server namespace: default spec: clusterIP: $CLUSTER_IP ports: - name: default port: 80 protocol: TCP targetPort: 80 selector: app: http-server type: ClusterIP EOF And apply it: kubectl apply -f service.yaml Now, let's create the ingress for this service: cat << EOF >> ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: labels: app: http-server name: http-server namespace: default spec: rules: - host: example.127.0.0.1.nip.io http: paths: - backend: serviceName: http-server servicePort: 80 pathType: ImplementationSpecific EOF And apply it: kubectl apply -f ingress.yaml You can check with CLI: kubectl get ingress Or using the Rancher, navigate to Load Balancing tab, you can see your ingress like this: Now, you can access the URL http://example.127.0.0.1.nip.io to access the deployment with ingress and it should looks like:","title":"Ingress"},{"location":"workloads/#wrap-up","text":"In this example, you can learn how to deploy a HTTP server and expose it for external access. Besides, with the K8s deployment, you can also scale the HTTP server to multiple instances which can handle large workloads with minimum operation efforts. There still a lot of topics to optimize the configuration to make it production ready. We might have more example in the future.","title":"Wrap Up"}]}